{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59e0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f18fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749a7ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363382ddb75240fd8eb47ef5b2f829f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d73f7f4c9543ad89ea3e3eeb03bb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa695c99919946bda2f031937e32d632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6d93d102a649f5a9e1e94e2ea31b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5626f746d794770b1584840d5f3076b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf620369f0e4a71a6770c48e2fd9c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inicializa el modelo de embedding específico\n",
    "# Esto descargará el modelo si es la primera vez que lo usas.\n",
    "embedding_model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483873db",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just discovered the course. Can I join now?'\n",
    "\n",
    "# El método embed devuelve un generador de embeddings.\n",
    "# Como es una sola consulta, tomamos el primer elemento.\n",
    "query_embedding = next(embedding_model.embed(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "666dd3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La consulta: 'I just discovered the course. Can I join now?'\n",
      "El embedding es un array NumPy con 512 dimensiones.\n",
      "El valor mínimo en el array del embedding es: -0.11726374368207196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convertir el resultado a un array NumPy explícitamente si no lo fuera ya (TextEmbedding ya devuelve np.ndarray)\n",
    "# Aunque 'query_embedding' ya será un array de NumPy\n",
    "query_embedding_array = np.array(query_embedding)\n",
    "\n",
    "# Calcular el valor mínimo del array\n",
    "min_value = query_embedding_array.min()\n",
    "\n",
    "print(f\"La consulta: '{query}'\")\n",
    "print(f\"El embedding es un array NumPy con {query_embedding_array.shape[0]} dimensiones.\")\n",
    "print(f\"El valor mínimo en el array del embedding es: {min_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0867879",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f164fb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "965fa4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding.dot(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99c34efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'Can I still join the course after the start date?'\n",
    "\n",
    "query_embedding_doc = next(embedding_model.embed(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e071a483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9008529058287051)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding.dot(query_embedding_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2b473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de Consulta 1 (primeros 5 elementos): [-0.07639464 -0.07305554  0.05865016  0.03926705 -0.0141797 ]\n",
      "Longitud de Consulta 1: 1.0000\n",
      "Embedding de Consulta 2 (primeros 5 elementos): [-0.05453042 -0.07834519  0.03136102  0.02342347 -0.03063215]\n",
      "Longitud de Consulta 2: 1.0000\n",
      "---\n",
      "La similitud de coseno entre la Consulta 1 y la Consulta 2 es: 0.9009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# Paso 1: Inicializar el modelo de embedding\n",
    "embedding_model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "\n",
    "# Paso 2: Generar el embedding para la Consulta 1\n",
    "query1 = 'I just discovered the course. Can I join now?'\n",
    "q1_embedding = next(embedding_model.embed(query1))\n",
    "\n",
    "# Paso 3: Generar el embedding para la Consulta 2\n",
    "query2 = 'Can I still join the course after the start date?'\n",
    "q2_embedding = next(embedding_model.embed(query2))\n",
    "\n",
    "# Paso 4: Calcular la similitud de coseno usando el producto punto\n",
    "# Asumimos que los embeddings están normalizados (longitud 1.0)\n",
    "cosine_similarity = q1_embedding.dot(q2_embedding)\n",
    "\n",
    "print(f\"Embedding de Consulta 1 (primeros 5 elementos): {q1_embedding[:5]}\")\n",
    "print(f\"Longitud de Consulta 1: {np.linalg.norm(q1_embedding):.4f}\")\n",
    "print(f\"Embedding de Consulta 2 (primeros 5 elementos): {q2_embedding[:5]}\")\n",
    "print(f\"Longitud de Consulta 2: {np.linalg.norm(q2_embedding):.4f}\")\n",
    "print(f\"---\")\n",
    "print(f\"La similitud de coseno entre la Consulta 1 y la Consulta 2 es: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f039c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando el modelo de embedding...\n",
      "Generando embedding para la consulta: 'Can I still join the course after the start date?'\n",
      "Generando embeddings para los documentos...\n",
      "\n",
      "Calculando similitud de coseno entre la consulta y cada documento:\n",
      "Documento 1:\n",
      "  Texto (fragmento): Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, t...\n",
      "  Similitud de Coseno: 0.8106\n",
      "\n",
      "Documento 2:\n",
      "  Texto (fragmento): Yes, we will keep all the materials after the course finishes, so you can follow the course at your ...\n",
      "  Similitud de Coseno: 0.8499\n",
      "\n",
      "Documento 3:\n",
      "  Texto (fragmento): The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and ho...\n",
      "  Similitud de Coseno: 0.7991\n",
      "\n",
      "Documento 4:\n",
      "  Texto (fragmento): You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud accou...\n",
      "  Similitud de Coseno: 0.6813\n",
      "\n",
      "Documento 5:\n",
      "  Texto (fragmento): Star the repo! Share it with friends if you find it useful ❣️\n",
      "Create a PR if you see you can improve...\n",
      "  Similitud de Coseno: 0.6739\n",
      "\n",
      "--- Resultados ordenados por similitud de coseno ---\n",
      "Documento 2 (Similitud: 0.8499):\n",
      "  Texto: Yes, we will keep all the materials after the course finishes, so you can follow the course at your ...\n",
      "\n",
      "Documento 1 (Similitud: 0.8106):\n",
      "  Texto: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, t...\n",
      "\n",
      "Documento 3 (Similitud: 0.7991):\n",
      "  Texto: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and ho...\n",
      "\n",
      "Documento 4 (Similitud: 0.6813):\n",
      "  Texto: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud accou...\n",
      "\n",
      "Documento 5 (Similitud: 0.6739):\n",
      "  Texto: Star the repo! Share it with friends if you find it useful ❣️\n",
      "Create a PR if you see you can improve...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# --- 1. Datos de entrada ---\n",
    "documents = [\n",
    "    {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I still join the course after the start date?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I follow the course after it finishes?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - When will the course start?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - What can I do before the course starts?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'How can we contribute to the course?',\n",
    "     'course': 'data-engineering-zoomcamp'}\n",
    "]\n",
    "\n",
    "query = 'Can I still join the course after the start date?'\n",
    "\n",
    "# --- 2. Inicializar el modelo de embedding ---\n",
    "print(\"Inicializando el modelo de embedding...\")\n",
    "embedding_model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "\n",
    "# --- 3. Generar el embedding de la consulta ---\n",
    "print(f\"Generando embedding para la consulta: '{query}'\")\n",
    "query_embedding = next(embedding_model.embed(query))\n",
    "\n",
    "# --- 4. Generar embeddings para cada documento ---\n",
    "print(\"Generando embeddings para los documentos...\")\n",
    "document_texts = [doc['text'] for doc in documents]\n",
    "# El método embed puede tomar una lista de textos y devolverá un generador\n",
    "document_embeddings = list(embedding_model.embed(document_texts))\n",
    "\n",
    "# --- 5. Calcular la similitud de coseno para cada documento ---\n",
    "print(\"\\nCalculando similitud de coseno entre la consulta y cada documento:\")\n",
    "results = []\n",
    "for i, doc_embedding in enumerate(document_embeddings):\n",
    "    # Ya que los embeddings de jinaai/jina-embeddings-v2-small-en están normalizados,\n",
    "    # el producto punto es directamente la similitud de coseno.\n",
    "    cosine_similarity = query_embedding.dot(doc_embedding)\n",
    "    results.append({\n",
    "        'document_index': i,\n",
    "        'text': documents[i]['text'][:100] + '...', # Mostrar solo un fragmento del texto\n",
    "        'similitud_coseno': cosine_similarity\n",
    "    })\n",
    "    print(f\"Documento {i+1}:\")\n",
    "    print(f\"  Texto (fragmento): {documents[i]['text'][:100]}...\")\n",
    "    print(f\"  Similitud de Coseno: {cosine_similarity:.4f}\\n\")\n",
    "\n",
    "# Opcional: Ordenar los resultados por similitud para ver los más relevantes primero\n",
    "results_sorted = sorted(results, key=lambda x: x['similitud_coseno'], reverse=True)\n",
    "\n",
    "print(\"--- Resultados ordenados por similitud de coseno ---\")\n",
    "for res in results_sorted:\n",
    "    print(f\"Documento {res['document_index']+1} (Similitud: {res['similitud_coseno']:.4f}):\")\n",
    "    print(f\"  Texto: {res['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28d310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento con mayor similitud es el Documento 2 (índice original 1).\n",
      "Su similitud de coseno es: 0.8499\n"
     ]
    }
   ],
   "source": [
    "# Suponiendo que `results_sorted` es la lista de diccionarios que obtuvimos antes\n",
    "# y que ya está ordenada en orden descendente por similitud.\n",
    "if results_sorted:\n",
    "    documento_mas_similar = results_sorted[0]\n",
    "    indice_original = documento_mas_similar['document_index']\n",
    "    similitud = documento_mas_similar['similitud_coseno']\n",
    "    print(f\"El documento con mayor similitud es el Documento {indice_original + 1} (índice original {indice_original}).\")\n",
    "    print(f\"Su similitud de coseno es: {similitud:.4f}\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19e7572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de document_embeddings: <class 'list'>\n",
      "Número de embeddings individuales: 5\n",
      "Forma del primer embedding: (512,)\n",
      "\n",
      "Usando np.stack():\n",
      "Tipo de la matriz resultante: <class 'numpy.ndarray'>\n",
      "Forma de la matriz resultante: (5, 512)\n",
      "Primeros 5 valores del primer embedding en la matriz apilada:\n",
      "[-0.02495248 -0.0396454  -0.00437673  0.02958302 -0.01203007]\n",
      "\n",
      "Usando np.array() directamente:\n",
      "Tipo de la matriz resultante: <class 'numpy.ndarray'>\n",
      "Forma de la matriz resultante: (5, 512)\n",
      "Primeros 5 valores del primer embedding en la matriz desde lista:\n",
      "[-0.02495248 -0.0396454  -0.00437673  0.02958302 -0.01203007]\n",
      "\n",
      "Generando embedding para la consulta: 'Can I still join the course after the start date?'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# Recreando el paso de obtención de embeddings (usando el modelo real)\n",
    "documents = [\n",
    "    {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I still join the course after the start date?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I follow the course after it finishes?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - When will the course start?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - What can I do before the course starts?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'How can we contribute to the course?',\n",
    "     'course': 'data-engineering-zoomcamp'}\n",
    "]\n",
    "\n",
    "query = 'Can I still join the course after the start date?'\n",
    "\n",
    "embedding_model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "document_texts = [doc['text'] for doc in documents]\n",
    "document_embeddings = list(embedding_model.embed(document_texts))\n",
    "\n",
    "print(f\"Tipo de document_embeddings: {type(document_embeddings)}\")\n",
    "print(f\"Número de embeddings individuales: {len(document_embeddings)}\")\n",
    "print(f\"Forma del primer embedding: {document_embeddings[0].shape}\\n\")\n",
    "\n",
    "# --- Métodos para crear una única matriz bidimensional ---\n",
    "\n",
    "# Método 1: Usando np.stack()\n",
    "# Ideal cuando quieres apilar arrays a lo largo de un nuevo eje (dimensión).\n",
    "# Asegura que cada embedding individual se mantenga como una \"fila\" en la nueva matriz.\n",
    "all_embeddings_stacked = np.stack(document_embeddings)\n",
    "\n",
    "print(f\"Usando np.stack():\")\n",
    "print(f\"Tipo de la matriz resultante: {type(all_embeddings_stacked)}\")\n",
    "print(f\"Forma de la matriz resultante: {all_embeddings_stacked.shape}\")\n",
    "# La forma será (número_de_documentos, dimensionalidad_del_embedding)\n",
    "# Por ejemplo, (5, 512) si tienes 5 documentos y embeddings de 512 dimensiones.\n",
    "print(f\"Primeros 5 valores del primer embedding en la matriz apilada:\\n{all_embeddings_stacked[0, :5]}\\n\")\n",
    "\n",
    "\n",
    "# Método 2: Usando np.array() directamente con la lista\n",
    "# Si todos los elementos de la lista son arrays NumPy de la misma forma,\n",
    "# np.array() los convertirá automáticamente en una matriz multidimensional.\n",
    "all_embeddings_from_list = np.array(document_embeddings)\n",
    "\n",
    "print(f\"Usando np.array() directamente:\")\n",
    "print(f\"Tipo de la matriz resultante: {type(all_embeddings_from_list)}\")\n",
    "print(f\"Forma de la matriz resultante: {all_embeddings_from_list.shape}\")\n",
    "# La forma será idéntica a np.stack(): (número_de_documentos, dimensionalidad_del_embedding)\n",
    "print(f\"Primeros 5 valores del primer embedding en la matriz desde lista:\\n{all_embeddings_from_list[0, :5]}\\n\")\n",
    "\n",
    "print(f\"Generando embedding para la consulta: '{query}'\")\n",
    "query_embedding = next(embedding_model.embed(query))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb68e132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8498906718659862)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings_from_list.dot(query_embedding).max()\n",
    "\n",
    "# V.dot(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8e153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cd466ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando el modelo de embedding...\n",
      "Generando embedding para la consulta: 'Can I still join the course after the start date?'\n",
      "Generando embeddings para los documentos...\n",
      "\n",
      "Calculando similitud de coseno entre la consulta y cada documento:\n",
      "Documento 1:\n",
      "  Texto (fragmento): Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, t...\n",
      "  Similitud de Coseno: 0.9273\n",
      "\n",
      "Documento 2:\n",
      "  Texto (fragmento): Yes, we will keep all the materials after the course finishes, so you can follow the course at your ...\n",
      "  Similitud de Coseno: 0.8595\n",
      "\n",
      "Documento 3:\n",
      "  Texto (fragmento): The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and ho...\n",
      "  Similitud de Coseno: 0.8397\n",
      "\n",
      "Documento 4:\n",
      "  Texto (fragmento): You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud accou...\n",
      "  Similitud de Coseno: 0.7632\n",
      "\n",
      "Documento 5:\n",
      "  Texto (fragmento): Star the repo! Share it with friends if you find it useful ❣️\n",
      "Create a PR if you see you can improve...\n",
      "  Similitud de Coseno: 0.7604\n",
      "\n",
      "--- Resultados ordenados por similitud de coseno ---\n",
      "Documento 1 (Similitud: 0.9273):\n",
      "  Texto: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, t...\n",
      "\n",
      "Documento 2 (Similitud: 0.8595):\n",
      "  Texto: Yes, we will keep all the materials after the course finishes, so you can follow the course at your ...\n",
      "\n",
      "Documento 3 (Similitud: 0.8397):\n",
      "  Texto: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and ho...\n",
      "\n",
      "Documento 4 (Similitud: 0.7632):\n",
      "  Texto: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud accou...\n",
      "\n",
      "Documento 5 (Similitud: 0.7604):\n",
      "  Texto: Star the repo! Share it with friends if you find it useful ❣️\n",
      "Create a PR if you see you can improve...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# --- 1. Datos de entrada ---\n",
    "documents = [\n",
    "    {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I still join the course after the start date?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - Can I follow the course after it finishes?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - When will the course start?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'Course - What can I do before the course starts?',\n",
    "     'course': 'data-engineering-zoomcamp'},\n",
    "    {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
    "     'section': 'General course-related questions',\n",
    "     'question': 'How can we contribute to the course?',\n",
    "     'course': 'data-engineering-zoomcamp'}\n",
    "]\n",
    "\n",
    "query = 'Can I still join the course after the start date?'\n",
    "\n",
    "# --- 2. Inicializar el modelo de embedding ---\n",
    "print(\"Inicializando el modelo de embedding...\")\n",
    "embedding_model = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "\n",
    "# --- 3. Generar el embedding de la consulta ---\n",
    "print(f\"Generando embedding para la consulta: '{query}'\")\n",
    "query_embedding = next(embedding_model.embed(query))\n",
    "\n",
    "# --- 4. Generar embeddings para cada documento ---\n",
    "print(\"Generando embeddings para los documentos...\")\n",
    "document_texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "# El método embed puede tomar una lista de textos y devolverá un generador\n",
    "document_embeddings = list(embedding_model.embed(document_texts))\n",
    "\n",
    "# --- 5. Calcular la similitud de coseno para cada documento ---\n",
    "print(\"\\nCalculando similitud de coseno entre la consulta y cada documento:\")\n",
    "results = []\n",
    "for i, doc_embedding in enumerate(document_embeddings):\n",
    "    # Ya que los embeddings de jinaai/jina-embeddings-v2-small-en están normalizados,\n",
    "    # el producto punto es directamente la similitud de coseno.\n",
    "    cosine_similarity = query_embedding.dot(doc_embedding)\n",
    "    results.append({\n",
    "        'document_index': i,\n",
    "        'text': documents[i]['text'][:100] + '...', # Mostrar solo un fragmento del texto\n",
    "        'similitud_coseno': cosine_similarity\n",
    "    })\n",
    "    print(f\"Documento {i+1}:\")\n",
    "    print(f\"  Texto (fragmento): {documents[i]['text'][:100]}...\")\n",
    "    print(f\"  Similitud de Coseno: {cosine_similarity:.4f}\\n\")\n",
    "\n",
    "# Opcional: Ordenar los resultados por similitud para ver los más relevantes primero\n",
    "results_sorted = sorted(results, key=lambda x: x['similitud_coseno'], reverse=True)\n",
    "\n",
    "print(\"--- Resultados ordenados por similitud de coseno ---\")\n",
    "for res in results_sorted:\n",
    "    print(f\"Documento {res['document_index']+1} (Similitud: {res['similitud_coseno']:.4f}):\")\n",
    "    print(f\"  Texto: {res['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05bb7cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El documento con mayor similitud es el Documento 1 (índice original 0).\n",
      "Su similitud de coseno es: 0.9273\n"
     ]
    }
   ],
   "source": [
    "if results_sorted:\n",
    "    documento_mas_similar = results_sorted[0]\n",
    "    indice_original = documento_mas_similar['document_index']\n",
    "    similitud = documento_mas_similar['similitud_coseno']\n",
    "    print(f\"El documento con mayor similitud es el Documento {indice_original + 1} (índice original {indice_original}).\")\n",
    "    print(f\"Su similitud de coseno es: {similitud:.4f}\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ef0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdcb175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "    if course_name != 'machine-learning-zoomcamp':\n",
    "        continue\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "491d2c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053116ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando embeddings...\n",
      "Indexando documentos...\n",
      "\n",
      "Buscando resultados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3898/3058370727.py:59: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Primer resultado devuelto ---\n",
      "ID: 14\n",
      "Score: 0.87031716\n",
      "Payload: {'text': 'Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.', 'question': 'The course has already started. Can I still join it?', 'course': 'machine-learning-zoomcamp'}\n"
     ]
    }
   ],
   "source": [
    "from fastembed import TextEmbedding\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# --- 1. Datos  ---\n",
    "\n",
    "\n",
    "query = \"I just discovered the course. Can I join now?\"\n",
    "\n",
    "# --- 2. Inicializar modelo de embedding ---\n",
    "model = TextEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "# --- 3. Preparar texto para embedding ---\n",
    "texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "\n",
    "# --- 4. Generar embeddings ---\n",
    "print(\"Generando embeddings...\")\n",
    "embeddings = list(model.embed(texts))\n",
    "query_embedding = next(model.embed(query))\n",
    "\n",
    "# Convertir a listas normales (list) desde tipos internos de fastembed\n",
    "embeddings_list = [emb.tolist() for emb in embeddings]\n",
    "query_vector = query_embedding.tolist()\n",
    "\n",
    "# --- 5. Conectar con Qdrant ---\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "collection_name = \"course_qa\"\n",
    "\n",
    "# Crear colección (si no existe)\n",
    "if not client.collection_exists(collection_name):\n",
    "    print(f\"Creando colección '{collection_name}'...\")\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=len(embeddings_list[0]), distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Subir vectores a Qdrant\n",
    "print(\"Indexando documentos...\")\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        {\n",
    "            \"id\": idx,\n",
    "            \"vector\": embedding,\n",
    "            \"payload\": {\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"question\": doc[\"question\"],\n",
    "                \"course\": doc[\"course\"]\n",
    "            }\n",
    "        }\n",
    "        for idx, (embedding, doc) in enumerate(zip(embeddings_list, documents))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- 6. Buscar en Qdrant ---\n",
    "print(\"\\nBuscando resultados...\")\n",
    "search_result = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=1  # Solo queremos el primer resultado\n",
    ")\n",
    "\n",
    "# --- 7. Mostrar el primer resultado y su score ---\n",
    "first_result = search_result[0]\n",
    "print(\"\\n--- Primer resultado devuelto ---\")\n",
    "print(\"ID:\", first_result.id)\n",
    "print(\"Score:\", first_result.score)\n",
    "print(\"Payload:\", first_result.payload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
